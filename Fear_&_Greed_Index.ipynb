{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- 1. INSTALL LIBRARIES ---\n",
        "# This part installs all the necessary packages for the project.\n",
        "print(\"--- Installing libraries... ---\")\n",
        "!pip install yfinance pytrends pandas scikit-learn plotly nbformat streamlit pyngrok joblib -q\n",
        "\n",
        "# --- 2. SETUP AND IMPORTS ---\n",
        "print(\"\\n--- Importing libraries and setting up... ---\")\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import joblib\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- 3. DATA COLLECTION & PREPROCESSING ---\n",
        "print(\"\\n--- Starting Data Collection & Preprocessing... ---\")\n",
        "# Fetch financial data\n",
        "tickers = ['^VIX', 'SPY']\n",
        "start_date = '2019-01-01'\n",
        "end_date = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "financial_data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "vix_data = financial_data['Close']['^VIX'].to_frame('VIX')\n",
        "spy_data = financial_data['Close']['SPY'].to_frame('SPY')\n",
        "daily_data = vix_data.join(spy_data).ffill()\n",
        "\n",
        "# Fetch Google Trends data\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "keywords = [\"stock market crash\", \"recession\", \"buy the dip\", \"market volatility\", \"inflation\"]\n",
        "timeframe = f'{start_date} {end_date}'\n",
        "trends_data = pd.DataFrame()\n",
        "for keyword in keywords:\n",
        "    try:\n",
        "        pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='', gprop='')\n",
        "        trend_df = pytrends.interest_over_time()\n",
        "        if not trend_df.empty:\n",
        "            trends_data[keyword] = trend_df[keyword]\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping '{keyword}': {e}\")\n",
        "if 'isPartial' in trends_data.columns:\n",
        "    trends_data = trends_data.drop(columns=['isPartial'])\n",
        "\n",
        "# Resample and align data\n",
        "weekly_data = daily_data.resample('W-MON').agg({'VIX': 'mean', 'SPY': 'last'}).ffill()\n",
        "df = weekly_data.join(trends_data, how='inner')\n",
        "\n",
        "# Create the Fear Index\n",
        "scaler = MinMaxScaler()\n",
        "fear_columns = [col for col in df.columns if col in keywords]\n",
        "df[fear_columns] = scaler.fit_transform(df[fear_columns])\n",
        "df['Fear_Index'] = df[fear_columns].mean(axis=1)\n",
        "\n",
        "# Create features and target variable\n",
        "df['SPY_Returns'] = df['SPY'].pct_change()\n",
        "df['Fear_Index_Lag1'] = df['Fear_Index'].shift(1)\n",
        "df['VIX_Rolling_Mean_4W'] = df['VIX'].rolling(window=4).mean()\n",
        "df['Target'] = (df['VIX'].shift(-1) > df['VIX_Rolling_Mean_4W'].shift(-1)).astype(int)\n",
        "df_model = df.dropna()\n",
        "print(\"--- Data processing complete. ---\")\n",
        "\n",
        "# --- 4. MODEL TRAINING ---\n",
        "print(\"\\n--- Training the predictive model... ---\")\n",
        "features = ['Fear_Index_Lag1', 'SPY_Returns']\n",
        "X = df_model[features]\n",
        "y = df_model['Target']\n",
        "split_index = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "print(\"--- Model training complete. ---\")\n",
        "\n",
        "# --- 5. SAVE THE MODEL AND SCALER ---\n",
        "print(\"\\n--- Saving model and scaler files... ---\")\n",
        "joblib.dump(gb_classifier, 'volatility_predictor_model.joblib')\n",
        "joblib.dump(scaler, 'google_trends_scaler.joblib')\n",
        "print(\"--- Files saved. ---\")\n",
        "\n",
        "# --- 6. DEFINE AND WRITE THE STREAMLIT APP FILE ---\n",
        "print(\"\\n--- Writing the Streamlit app file (app.py)... ---\")\n",
        "\n",
        "# Define the app code as a multi-line string\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import plotly.graph_objects as go\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "st.set_page_config(page_title=\"Volatility Fear & Greed Index\", page_icon=\"ðŸ“ˆ\", layout=\"wide\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = joblib.load('volatility_predictor_model.joblib')\n",
        "    scaler = joblib.load('google_trends_scaler.joblib')\n",
        "    return model, scaler\n",
        "\n",
        "try:\n",
        "    model, scaler = load_model()\n",
        "    model_loaded = True\n",
        "except FileNotFoundError:\n",
        "    model_loaded = False\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def get_live_data():\n",
        "    end_date = datetime.today()\n",
        "    start_date = end_date - timedelta(days=90)\n",
        "    financial_data = yf.download(['^VIX', 'SPY'], start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'), progress=False)\n",
        "    vix_data = financial_data['Close']['^VIX'].to_frame('VIX')\n",
        "    spy_data = financial_data['Close']['SPY'].to_frame('SPY')\n",
        "    daily_data = vix_data.join(spy_data).ffill()\n",
        "    weekly_data = daily_data.resample('W-MON').agg({'VIX': 'mean', 'SPY': 'last'}).ffill()\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    keywords = [\"stock market crash\", \"recession\", \"buy the dip\", \"market volatility\", \"inflation\"]\n",
        "    timeframe = f'{start_date.strftime(\"%Y-%m-%d\")} {end_date.strftime(\"%Y-%m-%d\")}'\n",
        "    trends_list = []\n",
        "    for keyword in keywords:\n",
        "        try:\n",
        "            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='', gprop='')\n",
        "            trend_df = pytrends.interest_over_time()\n",
        "            if not trend_df.empty and keyword in trend_df.columns:\n",
        "                trends_list.append(trend_df[[keyword]])\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not trends_list: return None, None\n",
        "    trends_data = pd.concat(trends_list, axis=1).ffill().bfill()\n",
        "    live_df = weekly_data.join(trends_data, how='inner')\n",
        "    return live_df, keywords\n",
        "\n",
        "st.title(\"ðŸ“ˆ Fear & Greed Index: Predicting Short-Term Volatility\")\n",
        "\n",
        "if not model_loaded:\n",
        "    st.error(\"Model files not found. Please ensure the notebook has run completely to generate the .joblib files.\")\n",
        "else:\n",
        "    st.header(\"ðŸ”® Volatility Prediction for Next Week\")\n",
        "    live_df, keywords = get_live_data()\n",
        "    if live_df is not None and not live_df.empty and len(live_df) > 1:\n",
        "        latest_data = live_df.iloc[[-1]]\n",
        "        fear_columns = [col for col in latest_data.columns if col in keywords]\n",
        "        # Ensure that fear_columns is not empty before proceeding\n",
        "        if fear_columns:\n",
        "            latest_data_scaled = scaler.transform(latest_data[fear_columns])\n",
        "            latest_data['Fear_Index'] = np.mean(latest_data_scaled, axis=1)\n",
        "            latest_data['SPY_Returns'] = live_df['SPY'].pct_change().iloc[-1]\n",
        "            features_for_prediction = pd.DataFrame({\n",
        "                'Fear_Index_Lag1': [latest_data['Fear_Index'].iloc[0]],\n",
        "                'SPY_Returns': [latest_data['SPY_Returns'].iloc[0]]\n",
        "            })\n",
        "            prediction = model.predict(features_for_prediction)[0]\n",
        "            prediction_proba = model.predict_proba(features_for_prediction)[0][1]\n",
        "            if prediction == 1:\n",
        "                st.error(f\"ðŸ”´ ALERT: High Volatility Expected (Probability: {prediction_proba:.1%})\", icon=\"ðŸš¨\")\n",
        "            else:\n",
        "                st.success(f\"ðŸŸ¢ NORMAL: Low Volatility Expected (Probability of Spike: {prediction_proba:.1%})\", icon=\"âœ…\")\n",
        "            st.markdown(f\"**Last Data Point:** Week of {latest_data.index[0].strftime('%Y-%m-%d')}\")\n",
        "            st.dataframe(latest_data[['VIX', 'Fear_Index', 'SPY_Returns']].style.format(\"{:.2f}\"))\n",
        "        else:\n",
        "            st.warning(\"Could not find any Google Trends keywords in the live data to create the Fear Index.\")\n",
        "    else:\n",
        "        st.warning(\"Could not fetch sufficient live data to make a prediction.\")\n",
        "    st.header(\"ðŸ“Š Live Data Visualization\")\n",
        "    if live_df is not None:\n",
        "        viz_df = live_df.copy()\n",
        "        fear_cols_viz = [col for col in viz_df.columns if col in keywords]\n",
        "        if fear_cols_viz:\n",
        "            viz_df_scaled = scaler.transform(viz_df[fear_cols_viz])\n",
        "            viz_df['Fear_Index'] = np.mean(viz_df_scaled, axis=1)\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(go.Scatter(x=viz_df.index, y=viz_df['VIX'], name='VIX Index', line=dict(color='royalblue')))\n",
        "            fig.add_trace(go.Scatter(x=viz_df.index, y=viz_df['Fear_Index'], name='Fear Index', line=dict(color='firebrick', dash='dash'), yaxis='y2'))\n",
        "            fig.update_layout(title=\"Live VIX vs. Fear Index (Last 90 Days)\", template='plotly_white', yaxis=dict(title='VIX Level'), yaxis2=dict(title='Fear Index', overlaying='y', side='right'), legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1))\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\"\"\"\n",
        "\n",
        "# Write the string to the app.py file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"--- App file written successfully. ---\")\n",
        "\n",
        "\n",
        "# --- 7. RUN THE STREAMLIT APP ---\n",
        "print(\"\\n--- Starting the dashboard... ---\")\n",
        "ngrok.kill()\n",
        "# Paste your ngrok token here\n",
        "AUTHTOKEN = \"2pLiaBO193VuGDE4XGFXw20j6y0_Te98D498t5ABemXS7hYY\"\n",
        "ngrok.set_auth_token(AUTHTOKEN)\n",
        "# Run streamlit in background\n",
        "!nohup streamlit run app.py &\n",
        "time.sleep(5)\n",
        "# Open a tunnel to the streamlit port\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"=\"*50)\n",
        "print(\"âœ… Your app is live! Click the link below to view it.\")\n",
        "print(public_url)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyi8-XN5vS-2",
        "outputId": "25fa1682-effc-47be-d3ac-44335bc75427"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing libraries... ---\n",
            "\n",
            "--- Importing libraries and setting up... ---\n",
            "\n",
            "--- Starting Data Collection & Preprocessing... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-2703809473.py:23: FutureWarning:\n",
            "\n",
            "YF.download() has changed argument auto_adjust default to True\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning:\n",
            "\n",
            "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning:\n",
            "\n",
            "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning:\n",
            "\n",
            "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning:\n",
            "\n",
            "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data processing complete. ---\n",
            "\n",
            "--- Training the predictive model... ---\n",
            "--- Model training complete. ---\n",
            "\n",
            "--- Saving model and scaler files... ---\n",
            "--- Files saved. ---\n",
            "\n",
            "--- Writing the Streamlit app file (app.py)... ---\n",
            "--- App file written successfully. ---\n",
            "\n",
            "--- Starting the dashboard... ---\n",
            "Downloading ngrok ...\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning:\n",
            "\n",
            "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "==================================================\n",
            "âœ… Your app is live! Click the link below to view it.\n",
            "NgrokTunnel: \"https://eb58d39b21ef.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}